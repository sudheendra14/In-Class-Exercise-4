{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sudheendra Mayda  In_class_exercise_04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/In_class_exercise/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total, 2/9/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "vR0L3_CreM_A",
        "outputId": "059af8c9-f9f1-42b8-a685-db067bc80ffa"
      },
      "source": [
        "# Write your code here\n",
        "import glob\n",
        "import re\n",
        "import csv\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "file = open('/content/01-05-1 Adams v Tanner.txt', 'r') \n",
        "\n",
        "sentenceCount = 0\n",
        "wordCount = 0\n",
        "char_count = 0\n",
        "total_char=0\n",
        "specialCharCount=0\n",
        "numberCount =0\n",
        "countOfStopWords =0\n",
        "upperCaseCount =0\n",
        "listOfStopWords=stopwords.words('english')\n",
        "Lines = file.readlines() \n",
        "for line in Lines:\n",
        "  sent = line.split(\".\")\n",
        "  for sentence in sent:\n",
        "    if len(sentence)>0:\n",
        "      filtered = ''.join(filter(lambda x: x not in '\".,;!-', sentence))\n",
        "      words_without_punc = [word for word in filtered.split() if word]\n",
        "      total_char = total_char + sum(map(len, words_without_punc))\n",
        "      wordCount = wordCount + sum([i.strip(string.punctuation).isalnum() for i in sentence.split()])\n",
        "      sentenceCount = sentenceCount+1\n",
        "      words = sentence.split(\" \")\n",
        "      for w in words:\n",
        "        new = re.sub('[\\w]+' ,'', w)\n",
        "        specialCharCount = specialCharCount + len(new)\n",
        "        if w.isnumeric()==True:\n",
        "          numberCount = numberCount+1\n",
        "        if w in listOfStopWords:\n",
        "          countOfStopWords = countOfStopWords + 1\n",
        "        if w.isupper()==True:\n",
        "          upperCaseCount=upperCaseCount+1\n",
        "file = open('/content/01-05-1 Adams v Tanner.txt', 'r') \n",
        "char_count = len(file.read())\n",
        "print(\"count of sentences=\", sentenceCount)\n",
        "print(\"count of words=\", wordCount)\n",
        "print(\"count of characters=\", char_count)\n",
        "print(\"average length of words is=\", total_char/wordCount)\n",
        "print(\"count of special characters is=\", specialCharCount)\n",
        "print(\"number of numerics=\", numberCount)\n",
        "print(\"stop word count =\", countOfStopWords)\n",
        "print(\"count of upper case words =\", upperCaseCount)\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "import pandas as pd\n",
        "from textblob import Word\n",
        "from textblob import TextBlob\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "content= open('/content/01-05-1 Adams v Tanner.txt', 'r').read()\n",
        "sentences=nltk.tokenize.sent_tokenize(content)\n",
        "Data={'sentences':sentences}\n",
        "f=pd.DataFrame(data=Data)\n",
        "\n",
        "f['sentences'] = f['sentences'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "ans1=f['sentences'].head()\n",
        "f['sentences'] = f['sentences'].str.replace('[^\\w\\s]','')\n",
        "ans2=f['sentences'].head()\n",
        "f['sentences'] = f['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in listOfStopWords))\n",
        "ans3=f['sentences'].head()\n",
        "Frequent = pd.Series(' '.join(f['sentences']).split()).value_counts()[:10]\n",
        "Frequent=list(Frequent.index)\n",
        "f['sentences'] = f['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in Frequent))\n",
        "ans4=f['sentences'].head()\n",
        "rare = pd.Series(' '.join(f['sentences']).split()).value_counts()[-10:]\n",
        "rare=list(rare.index)\n",
        "f['sentences'] = f['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n",
        "ans5=f['sentences'].head()\n",
        "f['sentences'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
        "ans6=f['sentences'].head()\n",
        "ans7=TextBlob(f['sentences'][1]).words\n",
        "stem = PorterStemmer()\n",
        "ans8=f['sentences'][:5].apply(lambda x: \" \".join([stem.stem(word) for word in x.split()]))\n",
        "f['sentences'] = f['sentences'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "ans9=f['sentences'].head()\n",
        "\n",
        "print(\"Lower casing \",ans1)\n",
        "print(\"Punctuation removal \",ans2)\n",
        "print(\"Stopwords removal\", ans3)\n",
        "print(\"Frequent words removal\", ans4)\n",
        "print(\"Rare words removal \",ans5)\n",
        "print(\"Spelling correction \",ans6)\n",
        "print(\"Tokenization \",ans7)\n",
        "print(\"Stemming \",ans8)\n",
        "print(\"Lemmatization\", ans9)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "f.to_csv('output_clean.csv',index=False)\n",
        "\n",
        "\n",
        "\n",
        "frequency = pd.Series(' '.join(f['sentences']).split()).value_counts()\n",
        "print(\"term frequency=\",frequency)\n",
        "TextBlob(f['sentences'][0]).ngrams(1)\n",
        "TextBlob(f['sentences'][0]).ngrams(2)\n",
        "TextBlob(f['sentences'][0]).ngrams(3)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9ffe2f4c432c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/01-05-1 Adams v Tanner.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msentenceCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/01-05-1 Adams v Tanner.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSv6fVhOfFmv",
        "outputId": "d2ef3138-83df-4347-e517-ef630d1716e0"
      },
      "source": [
        "# Write your code here\n",
        "ip=\"260.08.094.109\"\n",
        "def removezero(ip):  \n",
        "    n_ip = \".\".join([str (int(i)) for i in ip.split(\".\")])   \n",
        "    return n_ip \n",
        "\n",
        "print(removezero(ip))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xdJpDx9gjbX",
        "outputId": "b16d37db-0cad-4d0a-9df2-6fa48809c9f4"
      },
      "source": [
        "# Write your code here\r\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\r\n",
        "listOfWords = sentence.split(\" \")\r\n",
        "ans = []\r\n",
        "for element in listOfWords:\r\n",
        "  if element[0].isnumeric() == True:\r\n",
        "    if element[0]=='2':\r\n",
        "      word = \"\";\r\n",
        "      for character in element:\r\n",
        "        if character.isnumeric() == True:\r\n",
        "          word = word + character\r\n",
        "      ans.append(word)\r\n",
        "for year in ans:\r\n",
        "  print(year)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2010\n",
            "2010\n",
            "2019\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}